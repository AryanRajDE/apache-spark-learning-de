{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c81a553-b790-45d1-bfe4-16a98067802e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Overview\n",
    "This notebook shows how to create PySpark Dataframe and how to view the data in pyspark using different ways and options. \n",
    "\n",
    "#### **Contents :**\n",
    "\n",
    "- **What is a DataFrame in Spark**\n",
    "- **Setting up Spark Session**\n",
    "- **Dataframe Creation**\n",
    "    1. PySpark DataFrame from a list of rows\n",
    "    2. PySpark DataFrame with an explicit schema\n",
    "    3. PySpark DataFrame from a pandas DataFrame\n",
    "    4. Create Empty DataFrame from Empty RDD with Schema (StructType)\n",
    "    5. Create Empty DataFrame Directly with Schema\n",
    "    6. Create Empty DataFrame without Schema (no columns)\n",
    "    7. Convert Empty RDD to DataFrame\n",
    "- **Viewing Dataframe**\n",
    "    - Viewing the Dataframe via `df.show()`, `display(df)` and `df.head()`\n",
    "    - Setting spark conf for eager evaluation\n",
    "    - Viewing the dataframe row vertically\n",
    "    - Viewing DataFrame columns, schema and summary\n",
    "    - Viewing the dataframe via `Dataframe.collect()` , `DataFrame.take()` and `DataFrame.tail()`\n",
    "    - Convert PySpark Datframe into Pandas Dataframe\n",
    "- **DataFrame with SQL**\n",
    "\n",
    "\n",
    "This is a **Python** notebook so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` magic command. `Python`, `Scala(%scala)`, `SQL(%sql)`, `FileStore(%fs)` and `R(%r)` all are supported.\n",
    "\n",
    "**Spark Dataframe Documentation Link**\n",
    "- https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#DataFrame-Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63fabc28-b57a-40e5-ac22-9b9f66b71abf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### What is a DataFrame in Spark\n",
    "In Spark, DataFrames are the distributed collections of data, organized into rows and columns. Each column in a DataFrame has a name and an associated type. DataFrames are similar to traditional database tables, which are structured and concise. We can say that DataFrames are relational databases with better optimization techniques.\n",
    "\n",
    "Spark DataFrames can be created from various sources, such as Hive tables, log tables, external databases, or the existing RDDs. DataFrames allow the processing of huge amounts of data.\n",
    "\n",
    "Spark has an easy-to-use API for handling structured and unstructured data called **Dataframe**. Every DataFrame has a blueprint called a **Schema**. It can contain universal data types string types and integer types and the data types which are specific to spark such as struct type.\n",
    "\n",
    "---------\n",
    "##### **Why DataFrames?**\n",
    "When Apache Spark 1.3 was launched, it came with a new API called DataFrames that resolved the limitations of performance and scaling that occur while using RDDs.\n",
    "\n",
    "When there is not much storage space in memory or on disk, RDDs do not function properly as they get exhausted. Besides, Spark RDDs do not have the concept of schemaâ€”the structure of a database that defines its objects. RDDs store both structured and unstructured data together, which is not very efficient.\n",
    "\n",
    "RDDs cannot modify the system in such a way that it runs more efficiently. RDDs do not allow us to debug errors during the runtime. They store the data as a collection of Java objects.\n",
    "\n",
    "RDDs use serialization (converting an object into a stream of bytes to allow faster processing) and garbage collection (an automatic memory management technique that detects unused objects and frees them from memory) techniques. This increases the overhead on the memory of the system as they are very lengthy.\n",
    "\n",
    "This was when DataFrames were introduced to overcome the limitations Spark RDDs had. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22ecc479-d7f6-47c6-9477-2a8bc412f707",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Setting up Spark Session\n",
    "PySpark applications start with initializing SparkSession which is the entry point of PySpark as below. In case of running it in PySpark shell via pyspark executable, the shell automatically creates the session in the variable spark for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9e547f8-074a-4ff5-bb1a-04207720cea3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# create spark context\n",
    "# conf = SparkConf().setAppName('MySparkApp').setMaster('local')\n",
    "# sc = SparkContext.getOrCreate(conf=conf)\n",
    "# print(conf.get(\"spark.master\"))\n",
    "# print(conf.get(\"spark.app.name\"))\n",
    "\n",
    "# OR\n",
    "\n",
    "# create spark session\n",
    "spark = SparkSession.builder.appName('MySparkApp').master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aadaf08e-7d03-40fb-81c2-d16ecd05598d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dataframe Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fea64f2-805c-4feb-b78a-5b9138de90cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1. PySpark DataFrame from a list of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57e07f42-9865-43d6-b78a-f34e9b37dd49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c003da0d-e74b-48bb-a00b-9c561ec19c3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr></thead><tbody><tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01T12:00:00.000+0000</td></tr><tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02T12:00:00.000+0000</td></tr><tr><td>4</td><td>5.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03T12:00:00.000+0000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         2.0,
         "string1",
         "2000-01-01",
         "2000-01-01T12:00:00.000+0000"
        ],
        [
         2,
         3.0,
         "string2",
         "2000-02-01",
         "2000-01-02T12:00:00.000+0000"
        ],
        [
         4,
         5.0,
         "string3",
         "2000-03-01",
         "2000-01-03T12:00:00.000+0000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "a",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "b",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "c",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "d",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "e",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "\n",
    "display(df)\n",
    "display(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5358375-0f74-4339-9684-c941a42074b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2. PySpark DataFrame with an explicit schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9bd7762-e501-441b-beee-68c109accdec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.printSchema of DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]>"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr></thead><tbody><tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01T12:00:00.000+0000</td></tr><tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02T12:00:00.000+0000</td></tr><tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03T12:00:00.000+0000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         2.0,
         "string1",
         "2000-01-01",
         "2000-01-01T12:00:00.000+0000"
        ],
        [
         2,
         3.0,
         "string2",
         "2000-02-01",
         "2000-01-02T12:00:00.000+0000"
        ],
        [
         3,
         4.0,
         "string3",
         "2000-03-01",
         "2000-01-03T12:00:00.000+0000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "a",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "b",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "c",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "d",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "e",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "\n",
    "display(df.printSchema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "767a74a7-4017-4052-9a52-9686034e50cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3. PySpark DataFrame from a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df72554-e662-4a8d-b75d-17dbdbe0ae4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.printSchema of DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]>"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr></thead><tbody><tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01T12:00:00.000+0000</td></tr><tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02T12:00:00.000+0000</td></tr><tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03T12:00:00.000+0000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         2.0,
         "string1",
         "2000-01-01",
         "2000-01-01T12:00:00.000+0000"
        ],
        [
         2,
         3.0,
         "string2",
         "2000-02-01",
         "2000-01-02T12:00:00.000+0000"
        ],
        [
         3,
         4.0,
         "string3",
         "2000-03-01",
         "2000-01-03T12:00:00.000+0000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "a",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "b",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "c",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "d",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "e",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "display(df.printSchema)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb7f4de8-805a-4593-a610-2daf1d047775",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 4. Create Empty DataFrame from Empty RDD with Schema (StructType)\n",
    "In order to create an empty PySpark DataFrame manually with schema (column names & datatypes). First, [Create a schema using StructType and StructField](https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3909b7fb-9b0c-41f1-842b-b6f00c77e280",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('firstname', StringType(), True), StructField('middlename', StringType(), True), StructField('lastname', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "#Create Schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([StructField('firstname', StringType(), True),\n",
    "                     StructField('middlename', StringType(), True),\n",
    "                     StructField('lastname', StringType(), True)])\n",
    "                     \n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbbdfda0-e12f-44da-bf01-cd00bfd382d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now create the empty RDD and pass it to `createDataFrame()` of SparkSession along with the schema for column names & data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81385ff-048c-4f62-abaf-47a1b8591ae8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[119] at readRDDFromInputStream at PythonRDD.scala:435\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Creates Empty RDD using parallelize\n",
    "emptyRDD= spark.sparkContext.parallelize([])\n",
    "print(emptyRDD)\n",
    "\n",
    "#Create empty DataFrame from empty RDD\n",
    "df = spark.createDataFrame(emptyRDD, schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6a030b7-3676-48f4-83fd-1265c7396b8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 5. Create Empty DataFrame Directly with Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a800e9-c502-4a1c-8947-5a982e81fee0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Create empty DataFrame directly\n",
    "df = spark.createDataFrame([], schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdd9bfa7-ec6f-46a5-8c01-6293a9a44a68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 6. Create Empty DataFrame without Schema (no columns)\n",
    "To create empty DataFrame with out schema (no columns) just create a empty schema and use it while creating PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08514e7-b2e5-42fb-a101-b20e064dc891",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n\n"
     ]
    }
   ],
   "source": [
    "#Create empty DatFrame with no schema (no columns)\n",
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7df043a-7f7e-4e16-be95-0fdcf9fd2e46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 7. Convert Empty RDD to DataFrame\n",
    "We can also create empty DataFrame by converting empty RDD to DataFrame using `toDF()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ac51ab-7d36-4c14-8cf8-872df550d3b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Convert empty RDD to Dataframe\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b569559-0bdb-4f3f-b7ab-71f1b32fcc42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Viewing Data \n",
    "There are several ways to view the created pyspark dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c895dc5-a121-406a-a9da-f23ef1f7ae66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Viewing the Dataframe via `df.show()`, `display(df)` and `df.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6614c873-d3a3-43e7-a3f6-909638cd1a2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n+---+---+-------+----------+-------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26418375-6ca0-42c4-884c-b8f0e2c98b6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr></thead><tbody><tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01T12:00:00.000+0000</td></tr><tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02T12:00:00.000+0000</td></tr><tr><td>4</td><td>5.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03T12:00:00.000+0000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         2.0,
         "string1",
         "2000-01-01",
         "2000-01-01T12:00:00.000+0000"
        ],
        [
         2,
         3.0,
         "string2",
         "2000-02-01",
         "2000-01-02T12:00:00.000+0000"
        ],
        [
         4,
         5.0,
         "string3",
         "2000-03-01",
         "2000-01-03T12:00:00.000+0000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "a",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "b",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "c",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "d",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "e",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab9b54c3-7674-4b2c-9325-12bdcc3c72e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0))]"
     ]
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94f78123-59ff-4779-9a1b-738eb119f5fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Setting spark conf for eager evaluation\n",
    "Alternatively, we can enable `spark.sql.repl.eagerEval.enabled` configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter. The number of rows to show can be controlled via `spark.sql.repl.eagerEval.maxNumRows` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf28d014-e1ea-4bf9-b3a5-03d22dae0178",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "</table>\n",
       "only showing top 2 rows\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<table border='1'>\n<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n</table>\nonly showing top 2 rows\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.maxNumRows', 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5edc4a5-e025-42dc-9f0a-01f406f5648f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Viewing the dataframe row vertically\n",
    "Useful when rows data are too lengthy to be seen horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9185f078-a2a3-44b6-899a-d9c25758ae8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n a   | 1                   \n b   | 2.0                 \n c   | string1             \n d   | 2000-01-01          \n e   | 2000-01-01 12:00:00 \n-RECORD 1------------------\n a   | 2                   \n b   | 3.0                 \n c   | string2             \n d   | 2000-02-01          \n e   | 2000-01-02 12:00:00 \nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7e075ee-2a5e-4311-a8ad-507b750c2f80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Viewing DataFrame columns, schema and summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7648bec7-b4bf-4c15-b0c4-519fee37c842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: ['a', 'b', 'c', 'd', 'e']"
     ]
    }
   ],
   "source": [
    "# for viewing columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72d9fda-04a2-4d40-85a0-22a475d3398a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# for viewing schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac47b499-020f-440b-8608-23f172430178",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------+\n|summary|                 a|                 b|      c|\n+-------+------------------+------------------+-------+\n|  count|                 3|                 3|      3|\n|   mean|2.3333333333333335|3.3333333333333335|   null|\n| stddev|1.5275252316519468|1.5275252316519468|   null|\n|    min|                 1|               2.0|string1|\n|    25%|                 1|               2.0|   null|\n|    50%|                 2|               3.0|   null|\n|    75%|                 4|               5.0|   null|\n|    max|                 4|               5.0|string3|\n+-------+------------------+------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# for viewing summary of dataframe\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39277b26-b219-4452-a208-518c2e726988",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####  Viewing the dataframe via `Dataframe.collect()` , `DataFrame.take()` and `DataFrame.tail()`\n",
    "`DataFrame.collect()` collects the distributed data to the driver side as the local data in Python. \n",
    "\n",
    "Note that this can throw an **out-of-memory error** when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb5991b9-bff6-47ae-8064-4b50851ad7ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n Row(a=4, b=5.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"
     ]
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9313ccf7-ddc0-4448-aca7-39b7cd24cd9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To avoid throwing an **out-of-memory exception**, use `DataFrame.take()` or `DataFrame.tail()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406ca41e-248d-4b38-887e-e43c0c24e85e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[33]: [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0))]"
     ]
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d151a5b2-d93e-46ae-9b41-530a59a180ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Convert PySpark Datframe into Pandas Dataframe\n",
    "PySpark DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas API. \n",
    "\n",
    "Note that `DataFrame.toPandas()` also collects all data into the driver side that can easily cause an **out-of-memory-error** when the data is too large to fit into the driver side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdabc2bc-d686-4c50-a920-a443af651034",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>string1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>string2</td>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>2000-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>string3</td>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>2000-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n      <th>d</th>\n      <th>e</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2.0</td>\n      <td>string1</td>\n      <td>2000-01-01</td>\n      <td>2000-01-01 12:00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3.0</td>\n      <td>string2</td>\n      <td>2000-02-01</td>\n      <td>2000-01-02 12:00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>5.0</td>\n      <td>string3</td>\n      <td>2000-03-01</td>\n      <td>2000-01-03 12:00:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7b906f5-da3b-45b7-8d53-6da82352f4cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### DataFrame with SQL\n",
    "DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. \n",
    "For example, we can register the DataFrame as a table and run a SQL easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a053ed-2b52-406c-8020-af377cbc1908",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n|color| fruit| v1| v2|\n+-----+------+---+---+\n|  red|banana|  1| 10|\n| blue|banana|  2| 20|\n|  red|carrot|  3| 30|\n| blue| grape|  4| 40|\n|  red|carrot|  5| 50|\n|black|carrot|  6| 60|\n|  red|banana|  7| 70|\n|  red| grape|  8| 80|\n+-----+------+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "df_color = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "\n",
    "df_color.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0ebdc5-c519-4588-b113-194b11d95441",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|       8|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df_color.createOrReplaceTempView('colorTable')\n",
    "spark.sql('select count(*) from colorTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03c001e3-3b03-4225-8e4c-96cefb83aef5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "createDataframe",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
