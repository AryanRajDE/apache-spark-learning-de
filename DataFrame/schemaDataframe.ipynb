{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a5bc20b-12d1-4ef1-b2b6-529a541dfaea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Overview\n",
    "This notebook shows how to define schema for PySpark Dataframe using different various options.\n",
    "\n",
    "#### **Contents :**\n",
    "\n",
    "- **Spark Schema**\n",
    "- **StructType – Defines the Structure of the DataFrame**\n",
    "- **StructField – Defines the Metadata of the DataFrame column**\n",
    "- **Schema Creation**\n",
    "    1. Create DataFrame using PySpark StructType & StructField\n",
    "    2. Create Nested StructType object struct using StructType and StructField\n",
    "    3. Adding & Changing struct of the DataFrame\n",
    "    4. Using SQL ArrayType and MapType\n",
    "    5. Creating StructType object struct from JSON file\n",
    "    6. Creating StructType object struct from DDL String\n",
    "\n",
    "#### Possible Interview Question\n",
    "1. **How to create Schema in PySpark ? What are the other ways to create Schema in PySpark ?**\n",
    "    - we can create the schema of a dataframe via two ways : `StructType` & `StructField` and `DDL`\n",
    "\n",
    "2. **What is StructType and StructField in Spark Schema ?**\n",
    "    - `StructType` and `StructField` classes in PySpark are used to specify the custom schema to the DataFrame and create complex columns like nested struct, array, and map columns. \n",
    "\n",
    "3. **What if there is Header in Schema ?**\n",
    "    - Use `option.('SkipRows', 1)` while Read to skip the first row which will be Header in the data.\n",
    "     \n",
    "\n",
    "This is a **Python** notebook so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` magic command. `Python`, `Scala(%scala)`, `SQL(%sql)`, `FileStore(%fs)` and `R(%r)` all are supported.\n",
    "\n",
    "**Spark Dataframe Documentation Link**\n",
    "- https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#DataFrame-Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e754b05e-f4c4-4765-b7e5-3eb346b2333d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark Schema \n",
    "\n",
    "- By default, Spark infers the schema from the data, however, sometimes we may need to define our own schema (column names and data types), especially while working with unstructured and semi-structured data.\n",
    "\n",
    "- **Spark Schema** is the structure of the DataFrame or Dataset, we can define it using StructType class which is a collection of StructField.\n",
    "\n",
    "- The `StructType` and `StructField` classes in PySpark are used to specify the custom schema to the DataFrame and create complex columns like nested struct, array, and map columns.\n",
    "\n",
    "- Why Schema Creation is Essential -\n",
    "  - **Data Consistency:** Defining a schema ensures uniformity in data types across columns, preventing type conflicts during processing.\n",
    "  - **Data Validation:** Schema creation allows you to impose constraints, ensuring data integrity and eliminating invalid records.\n",
    "  - **Optimized Performance:** With a defined schema, Spark can allocate memory efficiently, leading to faster data processing.\n",
    "  - **Type Safety:** Explicit schema creation offers better type safety during runtime, reducing the risk of runtime errors.\n",
    "\n",
    "- In Spark, we can define the schema of a dataframe via two ways : \n",
    "  1. **StructType** and **StructField**  \n",
    "  2. **DDL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d0419be-c7f8-48a2-a8dd-3cb870005a84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### StructType – Defines the Structure of the DataFrame\n",
    "\n",
    "- PySpark provides `StructType` class from `pyspark.sql.types` to define the structure of the DataFrame.\n",
    "\n",
    "-  `StructType` is commonly used to define the schema when creating a DataFrame, particularly for structured data with fields of different data types.\n",
    "\n",
    "- `StructType` represents a schema, which is a collection or list of `StructField` objects. A StructType is essentially a list of fields, each with a name and data type, defining the structure of the DataFrame. It allows for the creation of nested structures and complex data types.\n",
    "\n",
    "- We can create complex schemas with nested structures by nesting StructType within other StructType objects, allowing you to represent hierarchical or multi-level data.\n",
    "\n",
    "-  When reading data from various sources, specifying a StructType as the schema ensures that the data is correctly interpreted and structured. This is important when dealing with semi-structured or schema-less data sources.\n",
    "\n",
    "- PySpark `printSchema()` method on the DataFrame shows StructType columns as struct.\n",
    "\n",
    "-----\n",
    "\n",
    "#### StructField – Defines the Metadata of the DataFrame column\n",
    "\n",
    "- `StructField` represents a field in the schema, containing metadata such as the name, data type, and nullable status of the field. \n",
    "\n",
    "- `StructField` define the column name(String), column type (DataType), nullable column (Boolean) and metadata (MetaData).\n",
    "\n",
    "- Each StructField object defines a single column in the DataFrame, specifying its name and the type of data it holds.\n",
    "\n",
    "- The `StructField` class is also part of `pyspark.sql.types`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "771bb49c-a419-4bb9-99a1-e57ed8fc5a82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1. Create DataFrame using PySpark StructType & StructField "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e1b631-12b2-443e-9d2e-d1fb8b067dce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import statement\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f6304c-1f3c-4713-ba1a-c52c8f92271e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# preparing data for creating the DataFrame\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "\n",
    "# creating schema with StructType and StructField\n",
    "schema = StructType([StructField(\"firstname\", StringType(), nullable=True),\n",
    "                     StructField(\"middlename\", StringType(), nullable=True),\n",
    "                     StructField(\"lastname\", StringType(), nullable=True),\n",
    "                     StructField(\"id\", StringType(), nullable=True),\n",
    "                     StructField(\"gender\", StringType(), nullable=True),\n",
    "                     StructField(\"salary\", IntegerType(), nullable=True)])\n",
    " \n",
    "# creating dataframe \n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# display the result \n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cc4f4d6-f038-48b1-b9e8-4b67d0cb7d10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2. Create Nested StructType object struct using StructType and StructField\n",
    "To define a nested StructType in PySpark, use inner StructTypes within StructFields. Each nested StructType is a collection of StructFields, forming a hierarchical structure for representing complex data within DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed6f57a-f012-4009-b8e7-bf823a706e51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+--------------------+-----+------+------+\n|name                |id   |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3100  |\n|{Michael, Rose, }   |40288|M     |4300  |\n|{Robert, , Williams}|42114|M     |1400  |\n|{Maria, Anne, Jones}|39192|F     |5500  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# defining nested structure data\n",
    "structureData = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "                 ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "                 ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "                 ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "                 ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)]\n",
    "\n",
    "# defining schema using nested StructType\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "# creating dataframe\n",
    "df2 = spark.createDataFrame(data=structureData, schema=structureSchema)\n",
    "\n",
    "# display result\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dcea533-9efe-4658-8421-4e2e70b714c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3. Adding & Changing struct of the DataFrame\n",
    "Using PySpark SQL function `struct()`, we can change the struct of the existing DataFrame and add a new StructType to it. \n",
    "\n",
    "The below example demonstrates how to copy the columns from one structure to another and adding a new column. \n",
    "[PySpark Column Class](https://sparkbyexamples.com/pyspark/pyspark-column-functions/) also provides some functions to work with the StructType column.\n",
    "\n",
    "In example it copies “gender“, “salary” and “id” to the new struct “otherInfo” and add’s a new column “Salary_Grade“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ae5e14-40ad-4d17-8c64-9d422c65bf29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- OtherInfo: struct (nullable = false)\n |    |-- identifier: string (nullable = true)\n |    |-- gender: string (nullable = true)\n |    |-- salary: integer (nullable = true)\n |    |-- Salary_Grade: string (nullable = false)\n\n+--------------------+------------------------+\n|name                |OtherInfo               |\n+--------------------+------------------------+\n|{James, , Smith}    |{36636, M, 3100, Medium}|\n|{Michael, Rose, }   |{40288, M, 4300, High}  |\n|{Robert, , Williams}|{42114, M, 1400, Low}   |\n|{Maria, Anne, Jones}|{39192, F, 5500, High}  |\n|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n+--------------------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Updating existing structtype using struct\n",
    "from pyspark.sql.functions import col,struct,when\n",
    "\n",
    "updatedDF = df2.withColumn(\"OtherInfo\", struct(col(\"id\").alias(\"identifier\"),\n",
    "                                               col(\"gender\").alias(\"gender\"),\n",
    "                                               col(\"salary\").alias(\"salary\"),\n",
    "                                               when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\")\n",
    "                                               .when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\")\n",
    "                                               .otherwise(\"High\").alias(\"Salary_Grade\"))\n",
    "                           ).drop(\"id\",\"gender\",\"salary\")\n",
    "\n",
    "updatedDF.printSchema()\n",
    "updatedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f36cae5-eb48-4de4-944f-b3332060e1e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 4. Using SQL ArrayType and MapType\n",
    "SQL StructType also supports `ArrayType` and `MapType` to define the DataFrame columns for array and map collections, respectively. In the below example, column hobbies defined as ArrayType(StringType) and properties defined as MapType(StringType,StringType) meaning both key and value as String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd2fdf66-b65a-4a88-90b0-2d902afbd8f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- hobbies: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----+-------+----------+\n|name|hobbies|properties|\n+----+-------+----------+\n+----+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using SQL ArrayType and MapType\n",
    "arrayStructureSchema = StructType([\n",
    "    StructField('name', StructType([\n",
    "       StructField('firstname', StringType(), True),\n",
    "       StructField('middlename', StringType(), True),\n",
    "       StructField('lastname', StringType(), True)\n",
    "       ])),\n",
    "       StructField('hobbies', ArrayType(StringType()), True),\n",
    "       StructField('properties', MapType(StringType(),StringType()), True)\n",
    "    ])\n",
    "\n",
    "df3 = spark.createDataFrame([], schema=arrayStructureSchema)\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91d840d2-2592-42ad-9ef6-c69916cb52fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 5. Creating StructType object struct from JSON file\n",
    "Alternatively, we can load the SQL StructType schema from JSON file. To make it simple, we will get the current DataFrmae schems using `df2.schema.json()`, store this in a file, and use it to create a schema from this JSON file.\n",
    "\n",
    "We can also use `df.schema.simpleString()`, this will return a relatively simpler schema format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7c5be7e-dc2f-4d63-b455-825aaeb287b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/2010_summary.csv</td><td>2010_summary.csv</td><td>7121</td><td>1728547018000</td></tr><tr><td>dbfs:/FileStore/tables/2010_summary_write/</td><td>2010_summary_write/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/2010_summary_write.csv/</td><td>2010_summary_write.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/2010_summary_write_02/</td><td>2010_summary_write_02/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/2011_summary.json</td><td>2011_summary.json</td><td>21301</td><td>1729515377000</td></tr><tr><td>dbfs:/FileStore/tables/2011_summary_write.json/</td><td>2011_summary_write.json/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/NewFile/</td><td>NewFile/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/RangeFile/</td><td>RangeFile/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/RangeText/</td><td>RangeText/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/RangeText.txt/</td><td>RangeText.txt/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/SparkText</td><td>SparkText</td><td>1761</td><td>1728552324000</td></tr><tr><td>dbfs:/FileStore/tables/SparkText.txt</td><td>SparkText.txt</td><td>513</td><td>1728828118000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/2010_summary.csv",
         "2010_summary.csv",
         7121,
         1728547018000
        ],
        [
         "dbfs:/FileStore/tables/2010_summary_write/",
         "2010_summary_write/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/2010_summary_write.csv/",
         "2010_summary_write.csv/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/2010_summary_write_02/",
         "2010_summary_write_02/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/2011_summary.json",
         "2011_summary.json",
         21301,
         1729515377000
        ],
        [
         "dbfs:/FileStore/tables/2011_summary_write.json/",
         "2011_summary_write.json/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/NewFile/",
         "NewFile/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/RangeFile/",
         "RangeFile/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/RangeText/",
         "RangeText/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/RangeText.txt/",
         "RangeText.txt/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/SparkText",
         "SparkText",
         1761,
         1728552324000
        ],
        [
         "dbfs:/FileStore/tables/SparkText.txt",
         "SparkText.txt",
         513,
         1728828118000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls FileStore/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2437f6ab-9026-430a-8faf-f07e9cc846a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"fields\":[{\"metadata\":{},\"name\":\"name\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"firstname\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"middlename\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"lastname\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"id\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"gender\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"salary\",\"nullable\":true,\"type\":\"integer\"}],\"type\":\"struct\"}\nWrote 596 bytes.\nOut[41]: True"
     ]
    }
   ],
   "source": [
    "# Using json() to load StructType\n",
    "print(df2.schema.json())\n",
    "json_write = df2.schema.json()\n",
    "\n",
    "json_string = json.dumps(json_write)\n",
    "\n",
    "dbutils.fs.put('/dbfs/FileStore/tables/schema.json', json_string, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd35632-3da9-40e5-a0ed-04163e03b905",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1017468163024808>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Loading json schema to create DataFrame\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m load_json \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/FileStore/tables/schema.json\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      4\u001B[0m schemaFromJson \u001B[38;5;241m=\u001B[39m StructType\u001B[38;5;241m.\u001B[39mfromJson(load_json)\n",
       "\u001B[1;32m      6\u001B[0m df4 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(spark\u001B[38;5;241m.\u001B[39msparkContext\u001B[38;5;241m.\u001B[39mparallelize(structureData),schemaFromJson)\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.9/json/__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n",
       "\u001B[1;32m    341\u001B[0m     s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mdecode(detect_encoding(s), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurrogatepass\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n",
       "\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n",
       "\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n",
       "\u001B[0;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    348\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONDecoder\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.9/json/decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n",
       "\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, s, _w\u001B[38;5;241m=\u001B[39mWHITESPACE\u001B[38;5;241m.\u001B[39mmatch):\n",
       "\u001B[1;32m    333\u001B[0m     \u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n",
       "\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03m    containing a JSON document).\u001B[39;00m\n",
       "\u001B[1;32m    335\u001B[0m \n",
       "\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 337\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    338\u001B[0m     end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n",
       "\u001B[1;32m    339\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m end \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(s):\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.9/json/decoder.py:355\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[0;34m(self, s, idx)\u001B[0m\n",
       "\u001B[1;32m    353\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscan_once(s, idx)\n",
       "\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\u001B[0;32m--> 355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj, end\n",
       "\n",
       "\u001B[0;31mJSONDecodeError\u001B[0m: Expecting value: line 1 column 1 (char 0)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)\nFile \u001B[0;32m<command-1017468163024808>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Loading json schema to create DataFrame\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m load_json \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/FileStore/tables/schema.json\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m schemaFromJson \u001B[38;5;241m=\u001B[39m StructType\u001B[38;5;241m.\u001B[39mfromJson(load_json)\n\u001B[1;32m      6\u001B[0m df4 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(spark\u001B[38;5;241m.\u001B[39msparkContext\u001B[38;5;241m.\u001B[39mparallelize(structureData),schemaFromJson)\n\nFile \u001B[0;32m/usr/lib/python3.9/json/__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    341\u001B[0m     s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mdecode(detect_encoding(s), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurrogatepass\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[0;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONDecoder\n\nFile \u001B[0;32m/usr/lib/python3.9/json/decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, s, _w\u001B[38;5;241m=\u001B[39mWHITESPACE\u001B[38;5;241m.\u001B[39mmatch):\n\u001B[1;32m    333\u001B[0m     \u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03m    containing a JSON document).\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \n\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 337\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    338\u001B[0m     end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n\u001B[1;32m    339\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m end \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(s):\n\nFile \u001B[0;32m/usr/lib/python3.9/json/decoder.py:355\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[0;34m(self, s, idx)\u001B[0m\n\u001B[1;32m    353\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscan_once(s, idx)\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m--> 355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj, end\n\n\u001B[0;31mJSONDecodeError\u001B[0m: Expecting value: line 1 column 1 (char 0)",
       "errorSummary": "<span class='ansi-red-fg'>JSONDecodeError</span>: Expecting value: line 1 column 1 (char 0)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Loading json schema to create DataFrame\n",
    "import json\n",
    "load_json = json.loads('/FileStore/tables/schema.json')\n",
    "schemaFromJson = StructType.fromJson(load_json)\n",
    "\n",
    "df4 = spark.createDataFrame(spark.sparkContext.parallelize(structureData),schemaFromJson)\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "600bfcc3-956b-467a-9e14-90ee7104c9d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 6. Creating StructType object struct from DDL String\n",
    "To create a StructType object, `struct`, from a `Data Definition Language (DDL)` string in PySpark, use `StructType.fromDDL()`. This method parses the DDL string and generates a StructType object that reflects the schema defined in the string.\n",
    "\n",
    "For example, ‘struct = StructType.fromDDL(“name STRING, age INT”)’ creates a StructType with two fields: ‘name’ of type ‘STRING’ and ‘age’ of type ‘INT’. This allows for dynamic schema creation based on DDL specifications, facilitating seamless integration with external systems or data sources where schema information is defined using DDL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba00b1d-2c66-4d99-afd2-ab4caa6cbdd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[46]: StructType([StructField('col1', StringType(), True), StructField('col2', IntegerType(), True), StructField('col3', TimestampType(), True)])"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as T      \n",
    "\n",
    "# Create StructType from DDL String\n",
    "ddlSchemaStr = \"`fullName` STRUCT<`first`: STRING, `last`: STRING, `middle`: STRING>,`age` INT,`gender` STRING\"\n",
    "\n",
    "ddl_schema_string = \"col1 string, col2 integer, col3 timestamp\"\n",
    "ddl_schema = T._parse_datatype_string(ddl_schema_string)\n",
    "ddl_schema\n",
    "\n",
    "# ddlSchema = StructType.fromDDL(ddlSchemaStr)\n",
    "# ddlSchema.printTreeString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72241653-b9a3-43b3-a3be-3b49a651e41e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1017468163024811,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "schemaDataframe",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
