{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61460cd2-f3ec-4442-a3f3-93786b0cbe3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Overview\n",
    "This notebook shows how to create RDD in pyspark using different ways and options. \n",
    "\n",
    "#### **Contents :**\n",
    "\n",
    "- **Setting up Spark Context**\n",
    "- **Create RDD [Resilient Distributed Datasets]**\n",
    "- **Two ways to create RDD**\n",
    "    1. Parallelized Collection\n",
    "    2. External Datasets\n",
    "1. RDD from List\n",
    "2. RDD from Tuple\n",
    "3. Empty RDD\n",
    "4. RDD from an external text file\n",
    "5. RDD from range() function\n",
    "6. RDD from existing RDD\n",
    "7. RDD from JSON data\n",
    "\n",
    "\n",
    "This is a **Python** notebook so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` magic command. `Python`, `Scala(%scala)`, `SQL(%sql)`, `FileStore(%fs)` and `R(%r)` all are supported.\n",
    "\n",
    "**Spark RDD Documentation Link**\n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7d24a3-ac1c-4701-9d4b-1bdef60cb6f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2522566250873381#setting/sparkui/1023-165912-6dwj0phh/driver-4690385990211672665\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2522566250873381#setting/sparkui/1023-165912-6dwj0phh/driver-4690385990211672665\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In Databricks, the cell has bydefault spark session. So we can run the pyspark code without creating any spark session or spark context.\n",
    "# checking default spark version\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52e0dd08-a9fb-40ee-8434-dd94b64f4d8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Setting up the SparkContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cbaa023-2504-46f5-b1d2-5dc42f1d07c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\nMySparkApp\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('MySparkApp').setMaster('local')\n",
    "print(conf.get(\"spark.master\"))\n",
    "print(conf.get(\"spark.app.name\"))\n",
    "\n",
    "# sc = SparkContext(conf=conf)\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95f55c4-88de-413c-9f4c-f4da25157bc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark session\n",
    "# spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f548b00-ee5c-42cb-a9ba-584cc3fdda7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create RDD [Resilient Distributed Datasets]\n",
    "\n",
    "RDDs are commonly created through the `parallelization of collections`, such as taking an existing collection from the driver program (e.g., Scala, Python) and providing it to the `SparkContextâ€˜s parallelize()` method. This method is used only for testing but not in real-time, as the entire data used to create RDD is available in the driver node, which is not ideal for production.\n",
    "\n",
    "The Resilient Distributed Datasets are the group of data items that can be stored in-memory on worker nodes. \n",
    "Basically RDD is a collection of elements, partitioned across the nodes of the cluster so that we can execute various parallel operations on it.\n",
    "- **Resilient :** Restore the data on failure.\n",
    "- **Distributed :** Data is distributed among different nodes.\n",
    "- **Dataset :** Group of data.\n",
    "\n",
    "There are two ways to create RDDs -\n",
    "1. **Parallelized Collection :** Parallelizing an existing data in the driver program. To create parallelized collection, call SparkContext's parallelize method on an existing collection in the driver program. Each element of collection is copied to form a distributed dataset that can be operated on in parallel.\n",
    "2. **External Datasets :** In Spark, the distributed datasets can be created from any type of storage sources supported by Hadoop such as HDFS, Cassandra, HBase and even our local file system. Referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop Input Format.\n",
    "\n",
    "`rdd = spark.sparkContext.parallelize()` creates an RDD. The parallelize method distributes the data across the nodes in the Spark cluster, creating a parallel collection.\n",
    "\n",
    "`rdd.collect()` retrieves all the elements of the RDD to the driver program (in this case, the Python script), allowing them to be printed. \n",
    "The `collect()` action should be used cautiously with large datasets as it brings all the data to the driver, and it may lead to out-of-memory issues for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8592ab28-0cb5-4b23-b616-4da954cc7939",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1. RDD from List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea8ae44-4cc9-484d-9c3f-3d03d319c38b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 8\nAction: First element: 1\n[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "list_data = [1, 2, 3, 4, 5]\n",
    "rdd_list = sc.parallelize(list_data)\n",
    "\n",
    "rddListCollect = rdd_list.collect()\n",
    "\n",
    "print(\"Number of Partitions: \" + str(rdd_list.getNumPartitions()))\n",
    "print(\"Action: First element: \" + str(rdd_list.first()))\n",
    "print(rddListCollect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2d932a5-178c-4064-b99e-1d1116da572e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2. RDD from Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee211fc4-1a55-4cd1-8a26-e144755b4605",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 8\nAction: First element: ('Java', 20000)\n[('Java', 20000), ('Python', 10000), ('Scala', 30000)]\n"
     ]
    }
   ],
   "source": [
    "tuple_data = [(\"Java\", 20000),(\"Python\", 10000),(\"Scala\", 30000)]\n",
    "rdd_tuple = sc.parallelize(tuple_data)\n",
    "\n",
    "rddTupleCollect = rdd_tuple.collect()\n",
    "\n",
    "print(\"Number of Partitions: \" + str(rdd_tuple.getNumPartitions()))\n",
    "print(\"Action: First element: \" + str(rdd_tuple.first()))\n",
    "print(rddTupleCollect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f774d6be-decd-48d2-af33-9fcff4f09f57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3. Empty RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa90d60-5194-4965-83bf-d798a8dd94b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is Empty RDD : True\nis Empty RDD : True\nNumber of Partitions: 0\n[]\n"
     ]
    }
   ],
   "source": [
    "emptyRDD = sc.emptyRDD()\n",
    "emptyRDD2 = sc.parallelize([])\n",
    "\n",
    "print(\"is Empty RDD : \" + str(emptyRDD.isEmpty()))\n",
    "print(\"is Empty RDD : \" + str(emptyRDD2.isEmpty()))\n",
    "\n",
    "rddEmptyCollect = emptyRDD.collect()\n",
    "print(\"Number of Partitions: \" + str(emptyRDD.getNumPartitions()))\n",
    "print(rddEmptyCollect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49ef9af3-5a29-44d4-948f-8cf0953a253e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 4. RDD from an external text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d75d45-0975-4450-8f4c-9e54ee0910d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apache Spark is a lightning-fast cluster computing framework designed for real-time processing. Spark is an open-source project from Apache Software Foundation. Spark overcomes the limitations of Hadoop MapReduce, and it extends the MapReduce model to be efficiently used for data processing. Spark is a market leader for big data processing. It is widely used across organizations in many ways. It has surpassed Hadoop by running 100 times faster in memory and 10 times faster on disks.', '', 'Apache Spark is a fast, flexible, and developer-friendly leading platform for large-scale SQL, machine learning, batch processing, and stream processing. It is essentially a data processing framework that has the ability to quickly perform processing tasks on very large data sets. It is also capable of distributing data processing tasks across multiple computers, either by itself or in conjunction with other distributed computing tools.', '', 'Apache Spark is a unified engine designed for large-scale distributed data processing, on premises in data centers or in the cloud. ', 'Spark provides in-memory storage for intermediate computations, making it much faster than Hadoop MapReduce. It incorporates libraries with composable APIs for machine learning (MLlib), SQL for interactive queries (Spark SQL), stream processing (Structured Streaming) for interacting with real-time data, and graph processing (GraphX). ', '', 'Apache Spark is an in-memory computing framework or data processing engine which is used to process data in batch and real-time across various clusters of computers using simple programming languages such as Scala, Python, R, Java and so on.', '', 'Apache Spark is a unified computing engine with sets of libraries for parallel data processing on computer cluster.']\n"
     ]
    }
   ],
   "source": [
    "rddFile = sc.textFile(\"/FileStore/tables/SparkText\")\n",
    "rddFileCollect = rddFile.collect()\n",
    "print(rddFileCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5fcfc8c-65ef-4537-8519-e81d3b8bfa5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dbfs:/FileStore/tables/SparkText', 'Apache Spark is a lightning-fast cluster computing framework designed for real-time processing. Spark is an open-source project from Apache Software Foundation. Spark overcomes the limitations of Hadoop MapReduce, and it extends the MapReduce model to be efficiently used for data processing. Spark is a market leader for big data processing. It is widely used across organizations in many ways. It has surpassed Hadoop by running 100 times faster in memory and 10 times faster on disks.\\n\\nApache Spark is a fast, flexible, and developer-friendly leading platform for large-scale SQL, machine learning, batch processing, and stream processing. It is essentially a data processing framework that has the ability to quickly perform processing tasks on very large data sets. It is also capable of distributing data processing tasks across multiple computers, either by itself or in conjunction with other distributed computing tools.\\n\\nApache Spark is a unified engine designed for large-scale distributed data processing, on premises in data centers or in the cloud. \\nSpark provides in-memory storage for intermediate computations, making it much faster than Hadoop MapReduce. It incorporates libraries with composable APIs for machine learning (MLlib), SQL for interactive queries (Spark SQL), stream processing (Structured Streaming) for interacting with real-time data, and graph processing (GraphX). \\n\\nApache Spark is an in-memory computing framework or data processing engine which is used to process data in batch and real-time across various clusters of computers using simple programming languages such as Scala, Python, R, Java and so on.\\n\\nApache Spark is a unified computing engine with sets of libraries for parallel data processing on computer cluster.\\n')]\n"
     ]
    }
   ],
   "source": [
    "# To read the entire content of a file as a single record, use the wholeTextFiles() method on sparkContext.\n",
    "\n",
    "rddWholeFile = sc.wholeTextFiles('/FileStore/tables/SparkText')\n",
    "rddWholeFileCollect = rddWholeFile.collect()\n",
    "print(rddWholeFileCollect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4df3099-058d-4932-b4f5-8c8a3c24d161",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 5. RDD from range() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00a1d65-8527-4261-994b-e0b3d8165c23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "rddRange = sc.parallelize(range(1, 6))\n",
    "rddRangeCollect = rddRange.collect()\n",
    "print(rddRangeCollect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9546ab59-35e5-4590-b082-c79b7e2e609b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 6. RDD from existing RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a46bad-29d5-4614-90ce-ded8f41a846b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# We can use transformations like map, flatmap, and filter() to create a new RDD from an existing one.\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5]) \n",
    "newRdd = rdd.map(lambda x: x * 2)\n",
    "\n",
    "rddNewCollect = newRdd.collect()\n",
    "print(rddNewCollect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e1ade6f-7ab9-4070-8433-b6bef59cd80a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 7. RDD from JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a29eb3-978e-447d-bc2c-d9712e1812d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Kumar', 'age': 39, 'city': 'New York'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# The input to parallelize() is a Python dictionary obtained by loading JSON data (json_data) using the json.loads() method.\n",
    "\n",
    "# Create RDD from JSON\n",
    "json_data = '{\"name\": \"Kumar\", \"age\": 39, \"city\": \"New York\"}' \n",
    "rddJson = sc.parallelize([json.loads(json_data)])\n",
    "\n",
    "rddJsonCollect = rddJson.collect()\n",
    "print(rddJsonCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ef5449c-7c36-4087-86a2-7374e2de13b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "createRdd",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
