{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "This notebook shows how to read and write the data in pyspark via Dataframe API using different ways and options. \n",
    "\n",
    "#### **Contents :**\n",
    "- Reading the file via Dataframe API\n",
    "- Read Modes in PySpark\n",
    "- Way 1 : Reading file via `DataFrameReader.format().load()` method \n",
    "- Way 2 : Reading file via `DataFrameReader.csv()` method \n",
    "- Read Multiple Files in PySpark\n",
    "- Reading CSV File Options \n",
    "- Writing CSV File Options\n",
    "- Save Modes in PySaprk\n",
    "- Way 1 : Writing file via `DataframeWriter.write().csv()` method \n",
    "- Way 2 : Writing file via `DataframeWriter.write().format().save()` method \n",
    "- Save Dataframes into Persistent Tables \n",
    "- Generic File Source Options\n",
    "\n",
    "This is a **Python** notebook so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` magic command. `Python`, `Scala(%scala)`, `SQL(%sql)`, `FileStore(%fs)` and `R(%r)` all are supported.\n",
    "\n",
    "**Input CSV File Used :**\n",
    "- https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/flight-data/csv/2010-summary.csv\n",
    "\n",
    "**Spark Read/Write Documentation Link**\n",
    "- https://spark.apache.org/docs/latest/sql-data-sources.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aefdbbc-37a3-4eff-81f6-1e25712a0796",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Reading the file via Dataframe API\n",
    "\n",
    "We can load the data in spark via `DataframeReader API` and can also write it via `DataframeWriter API`. To access DataframeReaderAPI we have to use `spark.read()` method. \n",
    "\n",
    "By utilizing `DataFrameReader.csv(\"path\")` or `DataFrameReader.format(\"csv\").load(\"path\")` methods, we can read a CSV file into a PySpark DataFrame. These methods accept a file path as their parameter.\n",
    "\n",
    "Parameters required while reading any file from `spark.read()` :\n",
    "1. **format (optional) :** Data file format such as csv, json, jdbc/odbc, table *(default format is parquet)*\n",
    "2. **option (optional) :** To set up different options for file reading such as *inferschema, mode, header, path*\n",
    "3. **schema (optional) :** To pass manual schema\n",
    "4. **load (required) :** Path where our data is residing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7475d875-76a5-4857-b33f-acde80709597",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Read Modes in PySpark \n",
    "In PySpark, when reading data into a DataFrame from external sources, you can specify a reading mode to control how the system should handle issues such as missing files, corrupt records, and schema mismatches. The available reading modes depend on the data source.\n",
    "\n",
    "| Mode  | Meaning |\n",
    "| ------------- | ------------- |\n",
    "| **FAILFAST**  | This mode fails the reading process if it encounters any malformed/corrupted data or schema mismatch. |\n",
    "| **DROPMALFORMED**  | This mode drops any row that contains malformed data (e.g., extra columns). |\n",
    "| **PARMISSVE**  | This mode is the default mode while reading the dataframe. In permissive mode, PySpark reads as much data as possible and stores corrupt records in a `“_corrupt_record”` column ans set the `null value` to all the corrupted fields. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0fb25b8-a0e3-4b0c-9fb5-4dd414561cd9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Way 1 : Reading file via `DataFrameReader.format().load()` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th></tr></thead><tbody><tr><td>United States</td><td>Romania</td><td>1</td></tr><tr><td>United States</td><td>Ireland</td><td>264</td></tr><tr><td>United States</td><td>India</td><td>69</td></tr><tr><td>Egypt</td><td>United States</td><td>24</td></tr><tr><td>Equatorial Guinea</td><td>United States</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "United States",
         "Romania",
         "1"
        ],
        [
         "United States",
         "Ireland",
         "264"
        ],
        [
         "United States",
         "India",
         "69"
        ],
        [
         "Egypt",
         "United States",
         "24"
        ],
        [
         "Equatorial Guinea",
         "United States",
         "1"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DEST_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/tables/2010_summary.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "# Display the result dataframe \n",
    "display(df.head(5))\n",
    "display(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5135ca32-e9ab-45a1-9e23-b6e4e36f197e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Way 2 : Reading file via `DataFrameReader.csv()` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34223421-5efe-45c2-8e43-3eabb0de78ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th></tr></thead><tbody><tr><td>DEST_COUNTRY_NAME</td><td>ORIGIN_COUNTRY_NAME</td><td>count</td></tr><tr><td>United States</td><td>Romania</td><td>1</td></tr><tr><td>United States</td><td>Ireland</td><td>264</td></tr><tr><td>United States</td><td>India</td><td>69</td></tr><tr><td>Egypt</td><td>United States</td><td>24</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DEST_COUNTRY_NAME",
         "ORIGIN_COUNTRY_NAME",
         "count"
        ],
        [
         "United States",
         "Romania",
         "1"
        ],
        [
         "United States",
         "Ireland",
         "264"
        ],
        [
         "United States",
         "India",
         "69"
        ],
        [
         "Egypt",
         "United States",
         "24"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Read CSV File\n",
    "df_read = spark.read.csv(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "# Display the result dataframe \n",
    "display(df_read.head(5))\n",
    "display(df_read.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2375c128-f567-4234-ae30-56231871ab9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Read Multiple Files in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf862087-01a8-48cd-97e9-466dbe76290e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_read = spark.read.option(\"inferSchema\", True) \\\n",
    "                .option(\"delimiter\", \",\") \\\n",
    "                .option(\"header\", True) \\\n",
    "                .csv(['/FileStore/tables/2010_summary.csv', '/FileStore/tables/2010_summary_write.csv'])\n",
    "\n",
    "## OR ##\n",
    "\n",
    "df_read = spark.read.format('csv') \\\n",
    "  .option(\"inferSchema\", True) \\\n",
    "  .option(\"header\", True) \\\n",
    "  .option(\"sep\", ',') \\\n",
    "  .load([\"/FileStore/tables/2010_summary_write.csv\", \"/FileStore/tables/2010_summary.csv\"])\n",
    "\n",
    "# Display the result dataframe \n",
    "display(df_read.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa76fb2c-c868-480f-9998-574a207a062f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Reading CSV File Options \n",
    "PySpark CSV dataset provides multiple options to work with CSV files. Below are some of the most important options explained with examples. We can either chain `option()` to use multiple options or use the alternate `options()` method.\n",
    "\n",
    "**Syntax**\n",
    "- option(self, key, value) # Using single options\n",
    "- options(self, **options) # Using multiple options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c05d221a-4274-48ff-8e38-4066a7d020ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 1. Header Option\n",
    "If we have a header with column names on our input file, we need to explicitly specify True for header option using option`(\"header\",True)` not mentioning this, the API treats header as a data record.\n",
    "\n",
    "This option is used to read the first line of the CSV file as column names. By default the value of this option is `False` , and all column types are assumed to be a `string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52f1aa51-5eb5-499f-9376-37b36df4d8d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th></tr></thead><tbody><tr><td>United States</td><td>Romania</td><td>1</td></tr><tr><td>United States</td><td>Ireland</td><td>264</td></tr><tr><td>United States</td><td>India</td><td>69</td></tr><tr><td>Egypt</td><td>United States</td><td>24</td></tr><tr><td>Equatorial Guinea</td><td>United States</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "United States",
         "Romania",
         "1"
        ],
        [
         "United States",
         "Ireland",
         "264"
        ],
        [
         "United States",
         "India",
         "69"
        ],
        [
         "Egypt",
         "United States",
         "24"
        ],
        [
         "Equatorial Guinea",
         "United States",
         "1"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DEST_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Read CSV File\n",
    "df_read = spark.read.option('header', True).csv(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "# Display the result dataframe \n",
    "display(df_read.head(5))\n",
    "display(df_read.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34c162a3-c149-44be-8071-551d8a28eaaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 2. Delimiter Option\n",
    "`delimiter` option is used to specify the column delimiter of the CSV file. By default, it is **comma (,)** character, but can be set to any character like **pipe(|)**, **tab (\\t)**, **space** using this option.\n",
    "\n",
    "*Quotes Option* : When we have a column with a delimiter that used to split the columns, use quotes option to specify the quote character, by default it is ” and delimiters inside quotes are ignored. but using this option you can set any character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17055b21-d13f-49b9-963d-cce3169a1fe6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th></tr></thead><tbody><tr><td>United States</td><td>Romania</td><td>1</td></tr><tr><td>United States</td><td>Ireland</td><td>264</td></tr><tr><td>United States</td><td>India</td><td>69</td></tr><tr><td>Egypt</td><td>United States</td><td>24</td></tr><tr><td>Equatorial Guinea</td><td>United States</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "United States",
         "Romania",
         "1"
        ],
        [
         "United States",
         "Ireland",
         "264"
        ],
        [
         "United States",
         "India",
         "69"
        ],
        [
         "Egypt",
         "United States",
         "24"
        ],
        [
         "Equatorial Guinea",
         "United States",
         "1"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DEST_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Read CSV File\n",
    "df_read = spark.read\\\n",
    "    .option('delimiter', ',')\\\n",
    "    .option('header', True)\\\n",
    "    .csv(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "# Display the result dataframe \n",
    "display(df_read.head(5))\n",
    "display(df_read.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4043b10-3c43-4283-9f8d-22579f835ca5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 3. InferSchema Option\n",
    "The default value set to this option is `False` when setting to `True` it automatically infers column types based on the data. Note that, it requires reading the data one more time to infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7fa004-2a4d-4c64-b0f2-c1b26e509505",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th></tr></thead><tbody><tr><td>DEST_COUNTRY_NAME</td><td>ORIGIN_COUNTRY_NAME</td><td>count</td></tr><tr><td>United States</td><td>Romania</td><td>1</td></tr><tr><td>United States</td><td>Ireland</td><td>264</td></tr><tr><td>United States</td><td>India</td><td>69</td></tr><tr><td>Egypt</td><td>United States</td><td>24</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DEST_COUNTRY_NAME",
         "ORIGIN_COUNTRY_NAME",
         "count"
        ],
        [
         "United States",
         "Romania",
         "1"
        ],
        [
         "United States",
         "Ireland",
         "264"
        ],
        [
         "United States",
         "India",
         "69"
        ],
        [
         "Egypt",
         "United States",
         "24"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Read CSV File using inferschema and delimiter\n",
    "df_read = spark.read.options(inferSchema='True',delimiter=',')\\\n",
    "  .csv(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "# Display the result dataframe \n",
    "display(df_read.head(5))\n",
    "display(df_read.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad66fe4c-9402-4c50-b3be-9f214a014dc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th></tr></thead><tbody><tr><td>DEST_COUNTRY_NAME</td><td>ORIGIN_COUNTRY_NAME</td><td>count</td></tr><tr><td>United States</td><td>Romania</td><td>1</td></tr><tr><td>United States</td><td>Ireland</td><td>264</td></tr><tr><td>United States</td><td>India</td><td>69</td></tr><tr><td>Egypt</td><td>United States</td><td>24</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DEST_COUNTRY_NAME",
         "ORIGIN_COUNTRY_NAME",
         "count"
        ],
        [
         "United States",
         "Romania",
         "1"
        ],
        [
         "United States",
         "Ireland",
         "264"
        ],
        [
         "United States",
         "India",
         "69"
        ],
        [
         "Egypt",
         "United States",
         "24"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Define read options\n",
    "options = {\n",
    "    \"inferSchema\": \"True\",\n",
    "    \"delimiter\": \",\"\n",
    "}\n",
    "\n",
    "# Read a CSV file with specified options\n",
    "df_read = spark.read.options(**options).csv(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "# Display the result dataframe \n",
    "display(df_read.head(5))\n",
    "display(df_read.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e715d2-28e1-454f-9b2b-6276e81952b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th></tr></thead><tbody><tr><td>DEST_COUNTRY_NAME</td><td>ORIGIN_COUNTRY_NAME</td><td>count</td></tr><tr><td>United States</td><td>Romania</td><td>1</td></tr><tr><td>United States</td><td>Ireland</td><td>264</td></tr><tr><td>United States</td><td>India</td><td>69</td></tr><tr><td>Egypt</td><td>United States</td><td>24</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DEST_COUNTRY_NAME",
         "ORIGIN_COUNTRY_NAME",
         "count"
        ],
        [
         "United States",
         "Romania",
         "1"
        ],
        [
         "United States",
         "Ireland",
         "264"
        ],
        [
         "United States",
         "India",
         "69"
        ],
        [
         "Egypt",
         "United States",
         "24"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Read a CSV file with chaining multiple options\n",
    "df_read = spark.read.option(\"inferSchema\",True) \\\n",
    "                .option(\"delimiter\",\",\") \\\n",
    "                .csv(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "# Display the result dataframe \n",
    "display(df_read.head(5))\n",
    "display(df_read.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26061582-be3f-444a-93f3-c0563981baaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 4. Specify Custom Schema Option\n",
    "Reading CSV files with a user-specified custom schema in PySpark involves defining the schema explicitly before loading the data. You can define the schema for the CSV file by specifying the column names and data types using the `StructType` and `StructField` classes. These are from the `pyspark.sql.types` module.\n",
    "\n",
    "Using a user-specified custom schema provides flexibility in handling CSV files with specific data types or column names, ensuring that the DataFrame accurately represents the data according to the user’s requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c7a1d44-f572-4ace-9842-6e2a35bceb3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th></tr></thead><tbody><tr><td>United States</td><td>Romania</td><td>1</td></tr><tr><td>United States</td><td>Ireland</td><td>264</td></tr><tr><td>United States</td><td>India</td><td>69</td></tr><tr><td>Egypt</td><td>United States</td><td>24</td></tr><tr><td>Equatorial Guinea</td><td>United States</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "United States",
         "Romania",
         1
        ],
        [
         "United States",
         "Ireland",
         264
        ],
        [
         "United States",
         "India",
         69
        ],
        [
         "Egypt",
         "United States",
         24
        ],
        [
         "Equatorial Guinea",
         "United States",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DEST_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "\n",
    "# Using custom schema\n",
    "schema = StructType() \\\n",
    "      .add(\"DEST_COUNTRY_NAME\",StringType(),True) \\\n",
    "      .add(\"ORIGIN_COUNTRY_NAME\",StringType(),True) \\\n",
    "      .add(\"count\",IntegerType(),True) \n",
    "      \n",
    "df_with_schema = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "# Display the result dataframe \n",
    "display(df_with_schema.head(5))\n",
    "display(df_with_schema.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "789326a5-73fe-4bb9-b7d4-b1cbdf212967",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Writing CSV File Options \n",
    "When writing a DataFrame to a CSV file in PySpark, you can specify various options to customize the output. These options can be set using the option() method of the DataFrameWriter class. Here’s how to use write options with a CSV file:\n",
    "\n",
    "1. **header:** Specifies whether to include a header row with column names in the CSV file. Example: option(\"header\", \"true\").\n",
    "2. **delimiter:** Specifies the delimiter to use between fields in the CSV file. Example: option(\"delimiter\", \",\").\n",
    "3. **quote:** Specifies the character used for quoting fields in the CSV file. Example: option(\"quote\", \"\\\"\").\n",
    "4. **escape:** Specifies the escape character used in the CSV file. Example: option(\"escape\", \"\\\\\").\n",
    "5. **nullValue:** Specifies the string to represent null values in the CSV file. Example: option(\"nullValue\", \"NA\").\n",
    "6. **dateFormat:** Specifies the date format to use for date columns. Example: option(\"dateFormat\", \"yyyy-MM-dd\").\n",
    "7. **mode:** Specifies the write mode for the output. Options include “overwrite”, “append”, “ignore”, and “error”. Example: option(\"mode\", \"overwrite\").\n",
    "8. **compression:** Specifies the compression codec to use for the output file. Example: option(\"compression\", \"gzip\"). This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate). CSV built-in functions ignore this option.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c278674-cd3e-4240-8394-834ed37f95b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Save Modes in PySpark\n",
    "\n",
    "Save operations can optionally take a SaveMode, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing an Overwrite, the data will be deleted before writing out the new data.\n",
    "\n",
    "We can specify different saving modes while writing PySpark DataFrame to disk. These saving modes specify how to write a file to disk.\n",
    "\n",
    "| Mode  | Meaning |\n",
    "| ------------- | ------------- |\n",
    "| **overwrite**  | overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.  |\n",
    "| **append**  | append mode means when saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.  |\n",
    "| **ignore**  | ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected not to save the contents of the DataFrame and not to change the existing data. This is similar to a `CREATE TABLE IF NOT EXISTS in SQL`  |\n",
    "| **error**  | error mode means when saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown. This is a `default` option. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f972125e-ed9e-4aa2-8963-f51cef113d2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Way 1 : Writing file via `DataframeWriter.write().csv()` method \n",
    "To write a PySpark DataFrame to a CSV file, you can use the `write.csv()` method provided by the `DataFrame API`. This method takes a path as an argument, where the CSV file will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de648ae1-417b-4148-854e-84015036d303",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th></tr></thead><tbody><tr><td>United States</td><td>Romania</td><td>1</td></tr><tr><td>United States</td><td>Ireland</td><td>264</td></tr><tr><td>United States</td><td>India</td><td>69</td></tr><tr><td>Egypt</td><td>United States</td><td>24</td></tr><tr><td>Equatorial Guinea</td><td>United States</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "United States",
         "Romania",
         1
        ],
        [
         "United States",
         "Ireland",
         264
        ],
        [
         "United States",
         "India",
         69
        ],
        [
         "Egypt",
         "United States",
         24
        ],
        [
         "Equatorial Guinea",
         "United States",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DEST_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Read a CSV file with chaining multiple options\n",
    "df_read = spark.read.option(\"inferSchema\", True) \\\n",
    "                .option(\"delimiter\", \",\") \\\n",
    "                .option(\"header\", True) \\\n",
    "                .csv(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "# Display the result dataframe \n",
    "display(df_read.head(5))\n",
    "display(df_read.printSchema())\n",
    "\n",
    "# Save and Write DataFrame to CSV File via write().csv() method \n",
    "df_read.write.option(\"header\",True) \\\n",
    "               .mode('ignore') \\\n",
    "               .csv(\"/FileStore/tables/2010_summary_write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "319ec517-5b65-44ae-a6d5-9ba06f6fe3e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Way 2 : Writing file via `DataframeWriter.write().format().save()` method \n",
    "To write a PySpark DataFrame to a CSV file, you can use the `format().save()` method provided by the `DataFrame API`. This method takes a path as an argument, where the CSV file will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48191e26-c48d-44df-8537-0d5023058220",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th></tr></thead><tbody><tr><td>United States</td><td>India</td><td>69</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "United States",
         "India",
         69
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DEST_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read a CSV file with chaining multiple options\n",
    "df_read = spark.read.option(\"inferSchema\", True) \\\n",
    "                .option(\"delimiter\", \",\") \\\n",
    "                .option(\"header\", True) \\\n",
    "                .csv(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "# Display the result dataframe \n",
    "df_less = df_read.where(df_read.ORIGIN_COUNTRY_NAME=='India')\n",
    "display(df_less)\n",
    "\n",
    "# Save and Write DataFrame to CSV File via format().save() method\n",
    "df_less.write.format(\"csv\").option(\"header\",True) \\\n",
    "                .mode('overwrite') \\\n",
    "                .save(\"/FileStore/tables/2010_summary_write.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a24d1c85-bb9f-4bac-838d-ac75128fe36a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/2010_summary.csv</td><td>2010_summary.csv</td><td>7121</td><td>1728547018000</td></tr><tr><td>dbfs:/FileStore/tables/2010_summary_write/</td><td>2010_summary_write/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/2010_summary_write.csv/</td><td>2010_summary_write.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/2010_summary_write_02/</td><td>2010_summary_write_02/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/NewFile/</td><td>NewFile/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/RangeFile/</td><td>RangeFile/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/RangeText/</td><td>RangeText/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/RangeText.txt/</td><td>RangeText.txt/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/SparkText</td><td>SparkText</td><td>1761</td><td>1728552324000</td></tr><tr><td>dbfs:/FileStore/tables/SparkText.txt</td><td>SparkText.txt</td><td>513</td><td>1728828118000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/2010_summary.csv",
         "2010_summary.csv",
         7121,
         1728547018000
        ],
        [
         "dbfs:/FileStore/tables/2010_summary_write/",
         "2010_summary_write/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/2010_summary_write.csv/",
         "2010_summary_write.csv/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/2010_summary_write_02/",
         "2010_summary_write_02/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/NewFile/",
         "NewFile/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/RangeFile/",
         "RangeFile/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/RangeText/",
         "RangeText/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/RangeText.txt/",
         "RangeText.txt/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/SparkText",
         "SparkText",
         1761,
         1728552324000
        ],
        [
         "dbfs:/FileStore/tables/SparkText.txt",
         "SparkText.txt",
         513,
         1728828118000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls FileStore/tables/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c490bbd1-68ad-414d-9f44-ee910cbc5c93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Save Dataframes to Persistent Table \n",
    "\n",
    "- DataFrames can also be saved as persistent tables into Hive metastore using the `saveAsTable` command. Spark will create a default local Hive metastore (using Derby) for you. `saveAsTable` will materialize the contents of the DataFrame and create a pointer to the data in the Hive metastore. \n",
    "\n",
    "- Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the table method on a SparkSession with the name of the table.\n",
    "\n",
    "- For file-based data source, e.g. text, parquet, json, etc. you can specify a custom table path via the path option, e.g. `df.write.option(\"path\", \"/some/path\").saveAsTable(\"t\")`. When the table is dropped, the custom table path will not be removed and the table data is still there. If no custom table path is specified, Spark will write data to a default table path under the warehouse directory. When the table is dropped, the default table path will be removed too.\n",
    "\n",
    "- **Bucketing, Sorting and Partitioning :** For file-based data source, it is also possible to bucket and sort or partition the output. Bucketing and sorting are applicable only to persistent tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f713d28-0575-4abe-bd91-e483556b04a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th></tr></thead><tbody><tr><td>United States</td><td>Romania</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "United States",
         "Romania",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DEST_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_read = spark.read.option(\"inferSchema\", True) \\\n",
    "                .option(\"delimiter\", \",\") \\\n",
    "                .option(\"header\", True) \\\n",
    "                .csv(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "# Display the result dataframe \n",
    "df_less = df_read.where(df_read.ORIGIN_COUNTRY_NAME=='Romania')\n",
    "display(df_less)\n",
    "\n",
    "(df_less.write.mode('append')\n",
    "    # .partitionBy(\"favorite_color\")\n",
    "    # .bucketBy(1, \"ORIGIN_COUNTRY_NAME\")\n",
    "    # .sortBy('count')\n",
    "    .saveAsTable(\"2010_Romania\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b1d425d-6dba-44b8-998f-39248aa58370",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Generic File Source Options\n",
    "\n",
    "These generic options/configurations are effective only when using file-based sources: parquet, orc, avro, json, csv, text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45d3a061-ba51-46d8-a06e-8d84d202b950",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1. Ignore Corrupt Files\n",
    "Spark allows you to use the configuration spark.sql.files.ignoreCorruptFiles or the data source option ignoreCorruptFiles to ignore corrupt files while reading data from files. When set to true, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a5bd5f-1a27-4be6-8907-9ea60bde96bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|                 _c0|                _c1|  _c2|\n+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n+--------------------+-------------------+-----+\n|                 _c0|                _c1|  _c2|\n+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# enable ignore corrupt files via the data source option\n",
    "test_corrupt_df0 = spark.read.option(\"ignoreCorruptFiles\", \"true\") \\\n",
    "                             .csv(\"/FileStore/tables/\")\n",
    "    \n",
    "display(test_corrupt_df0.show())\n",
    "\n",
    "# enable ignore corrupt files via the configuration\n",
    "spark.sql(\"set spark.sql.files.ignoreCorruptFiles=true\")\n",
    "\n",
    "test_corrupt_df1 = spark.read.csv(\"/FileStore/tables/\")\n",
    "\n",
    "display(test_corrupt_df1.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92bb1f7d-790a-4a7e-8428-bb44ab88d930",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2. Ignore Missing Files\n",
    "Spark allows you to use the configuration spark.sql.files.ignoreMissingFiles or the data source option ignoreMissingFiles to ignore missing files while reading data from files. Here, missing file really means the deleted file under directory after you construct the DataFrame. When set to true, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3c2258c-bd51-4573-ba8d-c8b5d860d97a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3. Path Glob Filter\n",
    "pathGlobFilter is used to only include files with file names matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bd0436f-e3cd-498d-84c4-36dba472c445",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|                 _c0|                _c1|  _c2|\n+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"/FileStore/tables\", format=\"csv\", pathGlobFilter=\"*.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1e208d5-e056-433e-b37e-57c80af6d6ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 4. Recursive File Lookup\n",
    "recursiveFileLookup is used to recursively load files and it disables partition inferring. Its default value is false. If data source explicitly specifies the partitionSpec when recursiveFileLookup is true, exception will be thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601855cf-bc59-4eee-b9eb-50fb68d3a5b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|                 _c0|                _c1|  _c2|\n+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "recursive_loaded_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"recursiveFileLookup\", \"true\")\\\n",
    "    .load(\"/FileStore/tables\")\n",
    "    \n",
    "recursive_loaded_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e7dd302-97eb-4a12-88cb-48647653de6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 5. Modified Time Path Filter\n",
    "modifiedBefore and modifiedAfter are options that can be applied together or separately in order to achieve greater granularity over which files may load during a Spark batch query. (Note that Structured Streaming file sources don’t support these options.) When a timezone option is not provided, the timestamps will be interpreted according to the Spark session timezone (spark.sql.session.timeZone).\n",
    "- **modifiedBefore:** an optional timestamp to only include files with modification times occurring before the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "- **modifiedAfter:** an optional timestamp to only include files with modification times occurring after the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95fb15bb-8e07-44c0-be37-c32cc1573734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|                 _c0|                _c1|  _c2|\n+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3481412086443986>:7\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m df_before\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Only load files modified after 06/01/2050 @ 08:30:00\u001B[39;00m\n",
       "\u001B[0;32m----> 7\u001B[0m df_after \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      8\u001B[0m                      \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m, inferSchema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, modifiedAfter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-10-14T08:30:00\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      9\u001B[0m df_after\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n",
       "\u001B[1;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
       "\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Unable to infer schema for CSV. It must be specified manually."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3481412086443986>:7\u001B[0m\n\u001B[1;32m      4\u001B[0m df_before\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Only load files modified after 06/01/2050 @ 08:30:00\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m df_after \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m                      \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m, inferSchema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, modifiedAfter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-10-14T08:30:00\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m df_after\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Unable to infer schema for CSV. It must be specified manually.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Unable to infer schema for CSV. It must be specified manually.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Only load files modified before 07/1/2050 @ 08:30:00\n",
    "df_before = spark.read.load(\"/FileStore/tables\",\n",
    "                     format=\"csv\", modifiedBefore=\"2024-10-14T05:30:00\")\n",
    "df_before.show()\n",
    "\n",
    "# Only load files modified after 06/01/2050 @ 08:30:00\n",
    "df_after = spark.read.load(\"/FileStore/tables\",\n",
    "                     format=\"csv\", inferSchema=True, modifiedAfter=\"2024-10-14T08:30:00\")\n",
    "df_after.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef9cf542-2f2e-451b-83cb-7c2776833347",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1816751205421307,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "readWriteCSV",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
